
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content=“width=800”>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
 
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    strongsmall {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: 700
    }

    smalll {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
    }

    stronghuge {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
    }

    huge {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 30px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="sjtu_icon.png">
  <title>Shihao Zou</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link rel="shortcut icon" href="image/favicon.ico" />
  <link rel="bookmark" href="image/favicon.ico" />
  <meta name="google-site-verification" content="3Pi5gRNVZ_uFXQ1gBBx91DHgGFC32ASIPVvSeEiTqz8" />
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Shihao Zou &nbsp;&nbsp;<font face="KaiTi" size="6">邹世豪</font>
                </name>
              </p>
              <p>I’m a Second-year PhD student at Institute of Artificial Intelligence, Beihang University. Currently I am a member of <a href="http://colalab.net/">CoLab</a> supervised by Prof. <a href="https://scholar.google.com/citations?user=-QtVtNEAAAAJ&hl=zh-CN">Si Liu</a>. I did my master study at University of Chinese Academy of Sciences under the supervision of Prof. <a href="https://scholar.google.com/citations?user=-QtVtNEAAAAJ&hl=zh-CN">Si Liu</a>. I earned my Bachelor degree from Xidian University, China.
              </p>
                I was a research intern at Peking University working with <a href="https://hughw19.github.io/">Wang He</a>, focusing on the Embodied AI research.
                I was a research intern at SenseTime and YITU Tech working with <a href="https://cypw.github.io/">Yunpeng Chen</a>, <a href="https://scholar.google.com/citations?user=DNuiPHwAAAAJ&hl=zh-CN">Shuicheng Yan</a>.
              <p>
                My research interests include Embodied Perception/Decision, Vision + Language and Generative Model.
              </p>
                <p align=center>
                  <a href="mailto:gaochen.ai@gmail.com">Email: gaochen.ai [at] gmail.com</a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=n5i2mAMAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://github.com/chengaopro">Github</a>
                </p>
            </td>
            <td width="16%">
              <img src="images/chengao.jpg" width="130">
            </td>
          </tr>
        </table>


        <p></p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>News</heading>
              <div style="line-height:25px">
                <p>
                  <li>
                    <em>(2023/03)</em> <strong>One</strong> paper is accepted in <strong>CVPR 2023</strong>.
                  <li>
                    <em>(2022/08)</em> I Got 华为Camera“学术之星” 奖学金.
                  <li>
                    <em>(2022/07)</em> <strong>One</strong> paper are accepted in <strong>ACM MM 2022</strong> (<font color="red">1 Oral Presentation</font>).
                  <li>
                    <em>(2022/06)</em> We won the <strong>first place</strong> in <a href="https://soundspaces.org/challenge">"SoundSpaces" Audio-visual Navigation Challenge</a> @CVPR Embodied AI Workshop 2022.
                  <li>
                    <em>(2022/03)</em> <strong>Two</strong> paper are accepted in <strong>CVPR 2022</strong> (<font color="red">1 Oral Presentation</font>).
                  <li>
                    <em>(2021/10)</em> <strong>One</strong> paper is accepted in <strong>NeurIPS 2021</strong>.
                  <li>
                    <em>(2021/07)</em> <strong>One</strong> paper is accepted in <strong>ICCV 2021</strong>.
                  <li>
                    <em>(2021/05)</em> <strong>One</strong> paper is accepted in <strong>TPAMI</strong>.
                  <li>
                    <em>(2021/03)</em> <strong>One</strong> paper is accepted in <strong>CVPR 2021</strong> (<font color="red">1 Oral Presentation</font>).
                  <li>
                    <em>(2020/07)</em> We won the <strong>first place</strong> in <a href="https://yuankaiqi.github.io/REVERIE_Challenge/">REVERIE Navigation Challenge</a> @ACL Workshop 2020.
                  <li>
                    <em>(2020/07)</em> <strong>One</strong> paper is accepted in <strong>ACM MM 2020</strong> (<font color="red">1 Oral Presentation</font>).
                  <li>
                    <em>(2020/02)</em> <strong>Two</strong> papers are accepted in <strong>CVPR 2020</strong> (<font color="red">1 Oral Presentation</font>).	
                  <li>
                    <em>(2019/05)</em> <strong>One</strong> papers is accepted in <strong>CHI 2019</strong>.			
                  <li>
                    <em>(2018/10)</em> We won the <strong>second place</strong> on Multi-Person Parsing of <a href="http://sysu-hcp.net/lip/">LIP Challenge</a> @CVPR Workshop 2018.
                </p>
              </div>
            </td>
          </tr>
        </table>

        <p></p>
        <p></p>
        <p></p>
        <p></p>
        <p></p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/AZHP.png' width="200" height="100">
            </td>
            <td valign="top" width="75%">
              <strong>Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation</strong>
              
              <br><br>
              <strong><u>Chen Gao</u></strong>,
              Xingyu Peng,
              Mi Yan,
              He Wang,
              Lirong Yang,
              Haibing Ren,
              Hongsheng Li,<br>
              Si Liu#,
              <br>
 
              <em>
                IEEE Conference on Computer Vision and Pattern Recognition. <strong>CVPR 2023</strong>.
              <br>
                <!-- <br> -->
                <a href=""><strong>[Paper]</strong></a> 
                <a href=""><strong>[Code]</strong></a>
                <br>

               <em>
              <p></p>
              <!-- <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable
                matching performance with acceptable model complexity and much less time consuming.
              </p> -->
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/VLN.png' width="200" height="80">
            </td>
            <td valign="top" width="75%">
              <strong>Target-Driven Structured Transformer Planner for Vision-Language Navigation</strong>
              <br><br>
              Yusheng Zhao*,
              Jinyu Chen*,
              <strong><u>Chen Gao</u></strong>,
              <a href="https://sites.google.com/view/wenguanwang">Wenguan Wang</a>,
              Lirong Yang,
              Haibin Ren,
              Huaxia Xia,
              Si Liu#,
              <br>
              <em>
                ACM International Conference on Multimedia. <strong>ACM MM 2022</strong>.
              <br>
              <em>
                <strong>
                  <font color="#a82e2e">(Oral Presentation)</font>
                </strong>
              </em>
              <br>
                <!-- <br> -->
                <a href="https://arxiv.org/pdf/2207.11201.pdf"><strong>[Paper]</strong></a> 
                <a href="https://github.com/YushengZhao/TD-STP"><strong>[Code]</strong></a>
                <br>

               <em>
              <p></p>
              <!-- <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable
                matching performance with acceptable model complexity and much less time consuming.
              </p> -->
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/SPS.png' width="200" height="100">
            </td>
            <td valign="top" width="75%">
              <strong>3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection</strong>
              
              <br><br>
              Junyu Luo*,
              Jiahui Fu*,
              Xianghao Kong,
              <strong><u>Chen Gao#</u></strong>,
              Haibing Ren,
              Hao Shen,
              Huaxia Xia,
              Si Liu,
              <br>
 
              <em>
                IEEE Conference on Computer Vision and Pattern Recognition. <strong>CVPR 2022</strong>.
              <br>
              <em>
                <strong>
                  <font color="#a82e2e">(Oral Presentation)</font>
                </strong>
              </em>
              <br>
                <!-- <br> -->
                <a href="https://arxiv.org/pdf/2204.06272.pdf"><strong>[Paper]</strong></a> 
                <a href="https://github.com/fjhzhixi/3D-SPS"><strong>[Code]</strong></a>
                <br>

               <em>
              <p></p>
              <!-- <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable
                matching performance with acceptable model complexity and much less time consuming.
              </p> -->
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/SEvol.png' width="200" height="107">
            </td>
            <td valign="top" width="75%">
              <strong>Reinforced Structured State-Evolution for Vision-Language Navigation</strong>
              <br><br>
              Jinyu Chen,
              <strong><u>Chen Gao</u></strong>,
              Erli Meng,
              Qiong Zhang,
              Si Liu#
              <br>
              <em>
                IEEE Conference on Computer Vision and Pattern Recognition. <strong>CVPR 2022</strong>.
              <br>
                <!-- <br> -->
                <a href="https://arxiv.org/pdf/2204.09280.pdf"><strong>[Paper]</strong></a> 
                <a href="https://github.com/chenjinyubuaa/SEvol"><strong>[Code]</strong></a>
                <br>

               <em>
              <p></p>
              <!-- <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable
                matching performance with acceptable model complexity and much less time consuming.
              </p> -->
            </td>
          </tr>
        </table>


        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/CDN.png' width="200" height="90">
            </td>
            <td valign="top" width="75%">
              <strong>Mining the Benefits of Two-stage and One-stage HOI Detection</strong>
              
              <br><br>
              Aixi Zhang*,
              Yue Liao*,
              Si Liu#,
              Miao Lu,
              Yongliang Wang,
              <strong><u>Chen Gao</u></strong>,
              Xiaobo Li
              <br>
 
              <em>
                Thirty-Fifth Conference on Neural Information Processing Systems. <strong>NeurIPS 2021</strong>.
              <br>
              </em>
                <a href="https://arxiv.org/pdf/2108.05077.pdf"><strong>[Paper]</strong></a> 
                <a href="https://github.com/YueLiao/CDN"><strong>[Code]</strong></a>
                <br>

               <em>
              <p></p>
            </td>
          </tr>
        </table> -->


        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/cm.png' width="200" height="150">
            </td>
            <td valign="top" width="75%">
              <strong>Language-Guided Global Image Editing via Cross-Modal Cyclic Mechanism</strong>
              
              <br><br>
              Wentao Jiang,
              Ning Xu,
              Jiayun Wang,
              <strong><u>Chen Gao</u></strong>,
              Shi Jing,
              <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
              Si Liu#,
              <br>
 
              <em>
                IEEE International Conference on Computer Vision. <strong>ICCV 2021</strong>.
              <br>
              </em>
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Language-Guided_Global_Image_Editing_via_Cross-Modal_Cyclic_Mechanism_ICCV_2021_paper.pdf"><strong>[Paper]</strong></a> 
                <br>

               <em>
              <p></p>
            </td>
          </tr>
        </table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/psgan++.png' width="200" height="96">
            </td>
            <td valign="top" width="75%">
              <strong>PSGAN++: Robust Detail-Preserving Makeup Transfer and Removal</strong>
              <br><br>
              Si Liu,
              Wentao Jiang,
              <strong><u>Chen Gao</u></strong>,
              Ran He,
              Jiashi Feng,
              Bo Li,
              Shuicheng Yan
              <br>
 
              <em>
                IEEE Transactions on Pattern Analysis and Machine Intelligence. <strong>TPAMI 2021</strong>.
              <br>
              </em>
                <a href="https://ieeexplore.ieee.org/abstract/document/9440729"><strong>[Paper]</strong></a> 
                <a href="https://github.com/wtjiang98/PSGAN"><strong>[Code & Dataset]</strong></a>
                  <br>
               <em>
              <p></p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/room.png' width="200" height="140">
            </td>
            <td valign="top" width="75%">
              <strong>Room-and-Object Aware Knowledge Reasoning for Remote Embodied Referring Expression</strong>
              
              <br><br>
              <strong><u>Chen Gao</u></strong>*,
              Jinyu Chen*,
              Si Liu#,
              Luting Wang,
              Qiong Zhang,
              Qi Wu
              <br>
              <em>
                IEEE Conference on Computer Vision and Pattern Recognition. <strong>CVPR 2021</strong>.
              <br>
              <em>
                <strong>
                  <font color="#a82e2e">(Oral Presentation)</font>
                </strong>
              </em>
              <br>
              </em>
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Room-and-Object_Aware_Knowledge_Reasoning_for_Remote_Embodied_Referring_Expression_CVPR_2021_paper.pdf"><strong>[Paper]</strong></a>
                <a href="https://github.com/alloldman/CKR"><strong>[Code]</strong></a>
                  <br>
               <em>
              <p></p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/adnas.png' width="200" height="103">
            </td>
            <td valign="top" width="75%">
              <strong>AdversarialNAS: Adversarial Neural Architecture Search for GANs</strong>
              
              <br><br>
              <strong><u>Chen Gao</u></strong>,
              Yunpeng Chen,
              Si Liu#,
              Zhenxiong Tan,
              Shuicheng Yan
              <br>
 
              <em>
                IEEE Conference on Computer Vision and Pattern Recognition. <strong>CVPR 2020</strong>.
                <br>
              </em>
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_AdversarialNAS_Adversarial_Neural_Architecture_Search_for_GANs_CVPR_2020_paper.pdf"><strong>[Paper]</strong></a>
                <a href="https://github.com/chengaopro/AdversarialNAS"><strong>[Code]</strong></a>
                  <br>
               <em>
              <p></p>
            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/psgan.gif' width="200" height="100">
            </td>
            <td valign="top" width="75%">
              <strong>PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer</strong>
              
              <br><br>
              Wentao Jiang,
              Si Liu#,
              <strong><u>Chen Gao</u></strong>,
              Jie Cao,
              Ran He,
              Jiashi Feng,
              Shuicheng Yan
              <br>
 
              <em>
                IEEE Conference on Computer Vision and Pattern Recognition. <strong>CVPR 2020</strong>.
                <br>
                <em>
                  <strong>
                    <font color="#a82e2e">(Oral Presentation)</font>
                  </strong></em>
                  <br>
              </em>
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_PSGAN_Pose_and_Expression_Robust_Spatial-Aware_GAN_for_Customizable_Makeup_CVPR_2020_paper.pdf"><strong>[Paper]</strong></a> 
                <a href="https://github.com/wtjiang98/PSGAN"><strong>[Code & Dataset]</strong></a>
                <br>
            
              <br>
              <p></p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/interactGAN.png' width="200" height="112">
            </td>
            <td valign="top" width="75%">
              <strong>InteractGAN: Learning to Generate Human-Object Interaction</strong>
              
              <br><br>
              <strong><u>Chen Gao</u></strong>,
              Si Liu#,
              Defa Zhu,
              Quan Liu,
              Jie Cao,
              Haoqian He,
              Ran He,
              Shuicheng Yan
              <br>
 
              <em>
                ACM International Conference on Multimedia. <strong>ACM MM 2020</strong>.
                <br>
                <em>
                  <strong>
                    <font color="#a82e2e">(Oral Presentation)</font>
                  </strong></em>
                  <br>
              </em>
                <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413854"><strong>[Paper]</strong></a>
                <a href="http://colalab.org/projects/InteractGAN"><strong>[Project]</strong></a>
                  <br>
               <em>
              <p></p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/pose.gif' width="200" height="100">
            </td>
            <td valign="top" width="75%">
              <strong>Attentive Transfer and Layout Graph Reasoning for Free-wheeling Portrait Recapturing</strong>
              
              <br><br>
              <strong><u>Chen Gao</u></strong>,
              Si Liu,
              Ran He,
              Shuicheng Yan
              <br>
 
              <em>
                arXiv preprint arXiv:2006.01435.
                <br>
              </em>
                <a href="https://arxiv.org/pdf/2006.01435.pdf"><strong>[Paper]</strong></a>
                  <br>
               <em>
              <p></p>
            </td>
          </tr>
        </table>

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/gps.png' width="200" height="100">
            </td>
            <td valign="top" width="75%">
              <strong>GPS: Group People Segmentation with Detailed Part Inference</strong>
              
              <br><br>
              Yue Liao,
              Si Liu,
              Tianrui Hui,
              <strong><u>Chen Gao</u></strong>,
              Yao Sun,
              Hefei Ling,
              Bo Li
              <br>
 
              <em>
                International Conference on Multimedia and Expo. <strong>ICME 2019</strong>.
                <br>
              </em>
               <em>
                <strong>
                  <font color="#a82e2e">(Oral Presentation)</font>
                </strong></em> <br> 
                <a href="http://colalab.org/media/paper/1231-camera-ready.pdf"><strong>[Paper]</strong></a>
              <p></p>
            </td>
          </tr>
        </table> -->
        
<!--   <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/chi.png' width="200" height="120">
            </td>
            <td valign="top" width="75%">
              <strong>Beyond Horror and Fear: Exploring Player Experience Invoked by Emotional Challenge in VR Games</strong>
              <br><br>
              Xiaolan Peng,
              Jin Huang,
              Linghan Li,
              <strong><u>Chen Gao</u></strong>,
              Hui Chen,
              Feng Tian,
              Hongan Wang
              <br>
              <em>
                The ACM CHI Conference on Human Factors in Computing Systems. <strong>CHI 2019</strong>.
                <br>
              </em>
                <a href="https://www.researchgate.net/profile/Xiaolan-Peng-2/publication/332774246_Beyond_Horror_and_Fear_Exploring_Player_Experience_Invoked_by_Emotional_Challenge_in_VR_Games/links/5d9022c0458515202b724115/Beyond-Horror-and-Fear-Exploring-Player-Experience-Invoked-by-Emotional-Challenge-in-VR-Games.pdf"><strong>[Paper]</strong></a>
              
                  <br><br> 

              <p></p>
            </td>
          </tr>
        </table> -->
        
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td width="15%">
              <img src='images/ugan.png' width="200" height="115">
            </td>
            <td valign="top" width="75%">
              <strong>UGAN: Untraceable GAN for Multi-Domain Face Translation</strong>
              
              <br><br>
              Defa Zhu,
              Si Liu,
              Wentao Jiang,
              <strong><u>Chen Gao</u></strong>,
              Tianyi Wu,
              Qaingchang W,
              Guodong Guo
              <br>
 
              <em>
                arXiv preprint arXiv:1907.11418, 2019
                <br>
              </em>
                <a href="https://arxiv.org/abs/1907.11418"><strong>[Paper]</strong></a>
                  <br>
               <em>
              <br>
              <p></p>
            </td>
          </tr>
        </table> -->


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Academic Services</heading>
              <div style="line-height:25px">
                <p>
                  <li>
                    Conference Reviewer: CVPR, ICCV, ECCV, NeurIPS, AAAI, ACM MM, ACCV, PRCV.<br />
                  <li>
                    Journal Reviewer: IEEE Transactions on Multimedia, IEEE Transactions on Signal and Information Processing over Networks, Multimedia Tools and Applications, Neurocomputing.<br />
                </p>
              </div>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Awards</heading>
              <div style="line-height:25px">
                <p>
                  <li>
                    National Scholarship, University of Chinese Academy of Sciences,&nbsp; 2020<br />
                  <li>
                    Outstanding Graduates, Xidian University (Top 1%),&nbsp;
                    2018<br />
                  <li>
                    National Scholarship, Xidian University (Top 1%),&nbsp; 2017<br />
                  <li>
                    National Scholarship, Xidian University (Top 1%),&nbsp; 2015<br />
                </p>
              </div>
            </td>
          </tr>
        </table>


        <center>
          <a href="https://clustrmaps.com/site/1bhe3"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=7jyPsD6c0yeMvDnTroUk7eWGtwLtuJgTu-xEVYc_DEk&cl=ffffff" /></a>
        </center>
</body>

</html>

